---
title: "Analysis of Data Science Job Postings"
author: "Benjamin Ackerman"
date: "September 1, 2017"
output: html_document
---

```{r setup, echo=FALSE, warning=FALSE,message=FALSE}
library(qdapRegex);library(knitr);library(dplyr);library(kableExtra);library(ggmap);library(leaflet)
```

Perform an analysis of "data scientist" jobs listed on job boards and on the employment pages of major companies. What are the most common skills that employers look for? What are the most unique skills that employers look for? Where are the types of companies that employ the most data scientists?

## 1. Scrape Data from Job Search Boards
I have scraped data from [Datajobs.com](https://datajobs.com/data-science-jobs).  They have a specific listing of data science jobs, which are formatted fairly consistently.  Here is my approach to obtain data:

- Create a vector of the pages that list jobs
- Extract job titles, company names, locations and links to job listings
    + From the job listing links, extract latitude and longitude from Google Maps figures, extract information on full vs part time jobs, and extract key skills (graduate degrees, coding experience, etc)
    
```{r scrape data from datajobs.com, echo=FALSE,eval=TRUE,cache=TRUE,warning=FALSE,results='asis'}
#Denote website name and pages
website = 'https://datajobs.com/'
pages = paste0("data-science-jobs",c("",paste0("~",2:30)))

links=jobs=companies=locations=list()
for(i in 1:length(pages)){
  # Read in the given list of web listings
  thepage = readLines(paste0(website,pages[i]))
  # Find the item in the html code that contains the websites on that page
  rawlistings = thepage[grep("Analytics and Data Science has become so valuable",thepage) + 6]
  # Extract the extensions of all of the job listings on that page
  pagenames<-unlist(rm_between(rawlistings, "href='", "'>", extract=TRUE))
  
  cleanerlistings=lapply(pagenames, function(x) rm_between(rawlistings, x, "</em></div></div><img src=", extract=TRUE))

  jobnames = unlist(rm_between(cleanerlistings, "<strong>","</strong>",extract=TRUE))
  companydata = rm_between(cleanerlistings, "' class='stealth-header'>","</span>",extract=TRUE)
  companyname = unlist(lapply(1:length(companydata), function(x) companydata[[x]][1]))
  location = sub(".*class='stealth-header'>", "", lapply(1:length(companydata), function(x) companydata[[x]][2]))
  
  # Get full URLs of the webpages
  listing_by_page=paste0(website,pagenames)
  
    links[[i]]=listing_by_page
    jobs[[i]]=jobnames
    companies[[i]]=companyname
    locations[[i]]=location
}

# Save information in a dataframe, remove missing rows and duplicate job postings
datajobs_postings = data.frame(position=unlist(jobs),company=unlist(companies),location=unlist(locations),links=unlist(links), stringsAsFactors = FALSE)
datajobs_postings = na.omit(datajobs_postings)
datajobs_postings<-datajobs_postings[!duplicated(datajobs_postings),]

# Create function to extract data from individual postings:
extract_listing<-function(x){
  job_posting = suppressWarnings(readLines(datajobs_postings$links[x]))
  employ_type = unlist(rm_between(job_posting[grep("<strong>Employment Type:</strong>",job_posting)+2],"\t\t\t\t\t","\t\t\t\t</div>",extract=TRUE))[1]
  if(nchar(employ_type) > 15){employ_type = unlist(rm_between(job_posting[grep("<strong>Employment Type:</strong>",job_posting)+2],"\t\t\t\t\t","<br",extract=TRUE))[1]}
  
  latlong = unlist(rm_between(job_posting[grep("google.maps.LatLng",job_posting)][1],"google.maps.LatLng(",")",extract=TRUE))
  latlong = as.numeric(unlist(strsplit(latlong,",")))
  c(employ_type,latlong)
}

# Apply function to all of the links in the dataframe
listing_dat<-as.data.frame(t(sapply(1:nrow(datajobs_postings),extract_listing)),stringsAsFactors=FALSE)
colnames(listing_dat) = c("employ_type","lat","long")

# Make latitude and longitude numeric
listing_dat[,c("lat","long")] = lapply(listing_dat[,c("lat","long")], function(x) as.numeric(x))

# Bind the individual listing data to the original dataset
datajobs_postings = cbind(datajobs_postings,listing_dat)

# Print the first ten job postings
kable(datajobs_postings[1:10,])
```

Above are the first 10 job listings from Data Jobs. I've identified `r nrow(datajobs_postings)` total unique job postings on Data Jobs.

I have also begun scraping data from [Stack Overflow](https://stackoverflow.com/jobs) by searching for jobs with the term "Data Scientist." Here's some of the data I've been able to extract:

```{r scrape data from stack overflow, echo=FALSE,eval=TRUE,cache=TRUE,warning=FALSE,results='asis'}
website = "https://stackoverflow.com/jobs?sort=i&q=data+scientist"
pages = c("",paste0("&pg=",c(2:6)))

links = list()
for(i in 1:length(pages)){
  thepage = readLines(paste0(website,pages)[i])
  rawlistings=thepage[grep("numberOfItems",thepage)]

  links[[i]] = unlist(rm_between(rawlistings, "url\":\"", "\"", extract=TRUE))
}
njobs = length(unlist(links))

about_job = data.frame(job_type=rep(NA,njobs), 
                       experience=rep(NA,njobs), 
                       role=rep(NA,njobs),
                       industry=rep(NA,njobs),
                       company_size=rep(NA,njobs),
                       company_type=rep(NA,njobs), 
                       tags = rep(NA,njobs),
                       link=unlist(links),
                       stringsAsFactors = FALSE)

for(j in 1:nrow(about_job)){
  listing = readLines(about_job$link[j])
  aboutjob.limit=grep("About this job",listing)[2]
  tags.data=listing[grep("post-tag job-link no-tag-menu",listing)][1]
  
  about_job$tags[j] = rm_between(tags.data,'developer-jobs-using-',"\"",extract=TRUE)
  
  descriptions<-unlist(rm_between(listing[aboutjob.limit:(aboutjob.limit+50)],"<span class=\"-key\">",":",extract=TRUE)) 
  descriptions<-descriptions[which(!is.na(descriptions))]
  
  descriptions[descriptions == "Job type"] = "job_type"
  descriptions[descriptions == "Experience level"] = "experience"
  descriptions[descriptions == "Role"] = "role"
  descriptions[descriptions == "Industry"] = "industry"
  descriptions[descriptions == "Company size"] = "company_size"
  descriptions[descriptions == "Company type"] = "company_type"

  responses<-unlist(rm_between(listing[aboutjob.limit:(aboutjob.limit+50)],"<span class=\"-value\">","</span>",extract=TRUE))
  responses<-responses[which(!is.na(responses))]
  
  about_job[j,descriptions]=responses
}
```

```{r print stackexchange, echo=FALSE, results='asis',cache=TRUE,eval=TRUE}
kable(about_job[1:10,-7])
```

There are `r nrow(about_job)` jobs listed on Stack Overflow that come up in my search.

## 2. Visualize Job Postings by Location
```{r plot jobs on map, echo=FALSE,message=FALSE,fig.align='center',fig.height=8,fig.width=10,cache=TRUE,warning=FALSE}
# map <- get_map(location = 'United States',zoom=4,maptype='roadmap')
# 
# ggmap(map) + geom_point(aes(x = long, y = lat,color='dark red',alpha=.5),size=3, data = datajobs_postings)+theme(legend.position="none")
m <- leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addMarkers(lng=datajobs_postings$long, lat=datajobs_postings$lat, popup = paste0("<b><a href='",datajobs_postings$links,"'>",datajobs_postings$company," - ",datajobs_postings$position,"</a></b>"))
m
```