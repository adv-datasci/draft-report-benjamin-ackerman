---
title: "Analysis of Data Science Job Postings"
author: "Benjamin Ackerman"
date: "September 26, 2017"
output: html_document
---

```{r setup, echo=FALSE, warning=FALSE,message=FALSE}
library(devtools);library(qdapRegex);library(knitr);library(dplyr);library(kableExtra);library(ggmap);library(leaflet);library(stringr)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library('RGlassdoor')
library(rvest)
library(jsonlite)
```

Perform an analysis of "data scientist" jobs listed on job boards and on the employment pages of major companies. What are the most common skills that employers look for? What are the most unique skills that employers look for? Where are the types of companies that employ the most data scientists?

## 1. Scrape Data from Job Search Boards
    
```{r scrape data from datajobs.com, echo=FALSE,eval=FALSE,cache=TRUE,warning=FALSE,results='asis'}
# I have scraped data from [Datajobs.com](https://datajobs.com/data-science-jobs).  They have a specific listing of data science jobs, which are formatted fairly consistently.  Here is my approach to obtain data:
# 
# - Create a vector of the pages that list jobs
# - Extract job titles, company names, locations and links to job listings
#     + From the job listing links, extract latitude and longitude from Google Maps figures, extract information on full vs part time jobs, and extract key skills (graduate degrees, coding experience, etc)

#Denote website name and pages
website = 'https://datajobs.com/'
pages = paste0("data-science-jobs",c("",paste0("~",2:30)))

links=jobs=companies=locations=list()
for(i in 1:length(pages)){
  # Read in the given list of web listings
  thepage = readLines(paste0(website,pages[i]))
  # Find the item in the html code that contains the websites on that page
  rawlistings = thepage[grep("Analytics and Data Science has become so valuable",thepage) + 6]
  # Extract the extensions of all of the job listings on that page
  pagenames<-unlist(rm_between(rawlistings, "href='", "'>", extract=TRUE))
  
  cleanerlistings=lapply(pagenames, function(x) rm_between(rawlistings, x, "</em></div></div><img src=", extract=TRUE))

  jobnames = unlist(rm_between(cleanerlistings, "<strong>","</strong>",extract=TRUE))
  companydata = rm_between(cleanerlistings, "' class='stealth-header'>","</span>",extract=TRUE)
  companyname = unlist(lapply(1:length(companydata), function(x) companydata[[x]][1]))
  location = sub(".*class='stealth-header'>", "", lapply(1:length(companydata), function(x) companydata[[x]][2]))
  
  # Get full URLs of the webpages
  listing_by_page=paste0(website,pagenames)
  
    links[[i]]=listing_by_page
    jobs[[i]]=jobnames
    companies[[i]]=companyname
    locations[[i]]=location
}

# Save information in a dataframe, remove missing rows and duplicate job postings
datajobs_postings = data.frame(position=unlist(jobs),company=unlist(companies),location=unlist(locations),links=unlist(links), stringsAsFactors = FALSE)
datajobs_postings = na.omit(datajobs_postings)
datajobs_postings<-datajobs_postings[!duplicated(datajobs_postings),]

# Create function to extract data from individual postings:
extract_listing<-function(x){
  job_posting = suppressWarnings(readLines(datajobs_postings$links[x]))
  employ_type = unlist(rm_between(job_posting[grep("<strong>Employment Type:</strong>",job_posting)+2],"\t\t\t\t\t","\t\t\t\t</div>",extract=TRUE))[1]
  if(nchar(employ_type) > 15){employ_type = unlist(rm_between(job_posting[grep("<strong>Employment Type:</strong>",job_posting)+2],"\t\t\t\t\t","<br",extract=TRUE))[1]}
  
  latlong = unlist(rm_between(job_posting[grep("google.maps.LatLng",job_posting)][1],"google.maps.LatLng(",")",extract=TRUE))
  latlong = as.numeric(unlist(strsplit(latlong,",")))
  c(employ_type,latlong)
  Sys.sleep(5)
}

# Apply function to all of the links in the dataframe
listing_dat<-as.data.frame(t(sapply(1:nrow(datajobs_postings),extract_listing)),stringsAsFactors=FALSE)
colnames(listing_dat) = c("employ_type","lat","long")

# Make latitude and longitude numeric
listing_dat[,c("lat","long")] = lapply(listing_dat[,c("lat","long")], function(x) as.numeric(x))

# Bind the individual listing data to the original dataset
datajobs_postings = cbind(datajobs_postings,listing_dat)
```

```{r datajob listings, echo=FALSE,eval=FALSE,cache=TRUE,warning=FALSE,results='asis'}
# Print the first ten job postings from Data Jobs
kable(datajobs_postings[1:10,])
Above are the first 10 job listings from Data Jobs. I've identified `r nrow(datajobs_postings)` total unique job postings on Data Jobs.

```


I have begun scraping data from [Stack Overflow](https://stackoverflow.com/jobs) by searching for jobs with the term "Data Scientist." Here's some of the data I've been able to extract:

```{r scrape data from stack overflow, echo=FALSE,eval=FALSE,cache=TRUE,warning=FALSE,results='asis'}
website = "https://stackoverflow.com/jobs?sort=i&q=data+scientist"
pages = c("",paste0("&pg=",c(2:7)))

links = list()
for(i in 1:length(pages)){
  thepage = readLines(paste0(website,pages)[i])
  rawlistings=thepage[grep("numberOfItems",thepage)]

  links[[i]] = unlist(rm_between(rawlistings, "url\":\"", "\"", extract=TRUE))
  Sys.sleep(4)
}
njobs = length(unique(unlist(links)))

about_job = data.frame(position = rep(NA,njobs),
                       company = rep(NA,njobs),
                       location = rep(NA,njobs),
                       job_type=rep(NA,njobs), 
                       experience=rep(NA,njobs), 
                       role=rep(NA,njobs),
                       industry=rep(NA,njobs),
                       company_size=rep(NA,njobs),
                       company_type=rep(NA,njobs), 
                       tags = rep(NA,njobs),
                       link=unique(unlist(links)),
                       stringsAsFactors = FALSE)

for(j in 1:nrow(about_job)){
  listing = readLines(about_job$link[j])
  aboutjob.limit=grep("About this job",listing)[2]
  tags.data=listing[grep("post-tag job-link no-tag-menu",listing)][1]
  
  about_job$tags[j] = rm_between(tags.data,'developer-jobs-using-',"\"",extract=TRUE)
  
  descriptions<-unlist(rm_between(listing[aboutjob.limit:(aboutjob.limit+50)],"<span class=\"-key\">",":",extract=TRUE)) 
  descriptions<-descriptions[which(!is.na(descriptions))]
  
  descriptions[descriptions == "Job type"] = "job_type"
  descriptions[descriptions == "Experience level"] = "experience"
  descriptions[descriptions == "Role"] = "role"
  descriptions[descriptions == "Industry"] = "industry"
  descriptions[descriptions == "Company size"] = "company_size"
  descriptions[descriptions == "Company type"] = "company_type"

  responses<-unlist(rm_between(listing[aboutjob.limit:(aboutjob.limit+50)],"<span class=\"-value\">","</span>",extract=TRUE))
  responses<-responses[which(!is.na(responses))]
  
  about_job[j,descriptions]=responses
  
  loc = listing[grep("<div class=\"-location\">",listing)[1]+2]
  loc=sub("</div>","",loc)
  about_job$location[j]=sub("\\s+$", "", loc)
  
  jobtitle =listing[grep("\"title\": \"",listing)]
  about_job$position[j] = unlist(rm_between(jobtitle, "\"title\": \"","\"",extract=TRUE))
  
  company =listing[grep("\"name\": \"",listing)]
  about_job$company[j] = unlist(rm_between(company, "\"name\": \"","\"",extract=TRUE))
  Sys.sleep(5)
}

# Clean up tags
about_job$tags = lapply(about_job$tags, function(x) gsub("-"," ", x))
about_job$tags = lapply(about_job$tags, function(x) gsub("%23","#", x))
about_job$tags = lapply(about_job$tags, function(x) gsub("%2b","+", x))

# Clean up location data
about_job$location=gsub("&#252;rn","urem",about_job$location)

# Geocode!
latlon = geocode(about_job$location,output='latlon')
# Bind geocodes to data
about_job = cbind(about_job,latlon)

# Functions to jitter latitude and longitude
length_of_degree <- function(degree, type = c("lat", "long")) {
  type <- match.arg(type)
  length_at_equator <- 110.5742727 # in kilometers
  if (type == "long") {
    cos(degree * (2 * pi) / 360) * length_at_equator
  } else if (type == "lat") {
    length_at_equator
  }
}
jitter_latlong <- function(coord, type = c("lat", "long"), latitude, km = 1) {
  type = match.arg(type)
  if(missing(latitude) & type == "lat") latitude <- coord
  km_per_degree <- length_of_degree(latitude, type = type)
  degree_per_km <- 1 / km_per_degree
  coord + (runif(1, min = -1, max = 1) * degree_per_km * km)
}
jitter_latlong <- Vectorize(jitter_latlong,
                            vectorize.args = c("coord", "latitude"))

# Jitter latitude and longitude
about_job$lat = jitter_latlong(about_job$lat,type='lat')
about_job$lon = jitter_latlong(about_job$lon,type='long',about_job$lat)

```

```{r stack overflow jobs, echo=FALSE, results='asis',cache=TRUE,eval=TRUE}
about_job = read.csv("../stackoverflowjob_2017-09-25.csv",header=TRUE,stringsAsFactors = FALSE)

kable(about_job[1:10,!names(about_job) %in% c("tags",'role',"lat","lon")])
```

There are `r nrow(about_job)` unique jobs listed on Stack Overflow that come up in my latest search.

In addition to the manual scraping I have performed, I have also received a database of jobs posted on Stack Overflow over the years from Dave Robinson, a Data Scientist at Stack Overflow.  Per his request, the data will be kept private.
```{r read and clean stack overflow data dump, eval=FALSE, echo=FALSE}
newdat<-readRDS('../data_scientist.rds')

# Split up the date into year, month, day
newdat[,c("year","month","day")]<-str_split(newdat$DatePosted,"-",simplify=TRUE)

# Clean Jobs that have multiple locataions 
repeats<-unique(newdat$JobId[which(duplicated(newdat$JobId))])
repeat_locations<-sapply(1:length(repeats),function(x){
   str_split(newdat$LocationString[which(newdat$JobId == repeats[x])],"; ",simplify=TRUE)[1,]
})
newdat$LocationString[which(newdat$JobId %in% repeats)] = unlist(repeat_locations)

# Geocode locations to plot them
test<-geocode(newdat$LocationString,output='latlon')

newdat$lat = jitter_latlong(test$lat,type='lat')
newdat$lon = jitter_latlong(test$lon,type='long',newdat$lat)

# Save updated dataset as "stackjobs.rds"
saveRDS(newdat,"../stackjobs.rds")
```

```{r save data on harddrive, echo=FALSE, eval=FALSE}
# Save current version of dataset to my own personal machine, date-stamped

# Data Jobs listings
#write.csv(datajobs_postings,file=paste0('/Users/backerman/Documents/JHSPH/3rd Year/Advanced Data Science/datajobs_',Sys.Date(),".csv"),row.names=FALSE,quote=TRUE)

about_job2 = about_job
about_job2$tags = unlist(lapply(about_job2$tags, function(x) str_c(x,collapse="_")))

# Stack Overflow listings
write.csv(about_job2,file=paste0('/Users/backerman/Documents/JHSPH/3rd Year/Advanced Data Science/stackoverflowjob_',Sys.Date(),".csv"),row.names=FALSE,quote=TRUE)
```

```{r load stack overflow data dump, echo=FALSE}
# Read in edited data
stackjobs = readRDS("../stackjobs.rds")
```

## 2. Visualize Job Postings by Location
```{r plot Data Jobs jobs on map,eval=FALSE, echo=FALSE,message=FALSE,fig.align='center',fig.height=8,fig.width=10,cache=TRUE,warning=FALSE}
# Here, I visualize job postings from Data Jobs, since latitude and longitude are very readily available from the job listings!

# map <- get_map(location = 'United States',zoom=4,maptype='roadmap')
# 
# ggmap(map) + geom_point(aes(x = long, y = lat,color='dark red',alpha=.5),size=3, data = datajobs_postings)+theme(legend.position="none")
m <- leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addMarkers(lng=datajobs_postings$long, lat=datajobs_postings$lat, popup = paste0("<b><a href='",datajobs_postings$links,"'>",datajobs_postings$company," - ",datajobs_postings$position,"</a></b>"))
m
```

```{r plot stack overflow jobs on map, eval=FALSE, echo=FALSE,message=FALSE,fig.align='center',fig.height=8,fig.width=10,cache=TRUE,warning=FALSE}
#Here, I visualize where jobs posted on Stack Overflow are located *using the data I scraped manually*.  
m <- leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addMarkers(lng=about_job$lon, lat=about_job$lat, popup = paste0("<b><a href='",about_job$link,"'>",about_job$company," - ",about_job$position,"</a></b>"),clusterOptions = markerClusterOptions())
m
```

Here, I visualize the Stack Overflow jobs using data provided from Dave Robinson.  Markers are color coded by the year that they were posted.
I use the function `geocode` from the `ggmaps` package to obtain latitude and longitude coordinates on the cities.  I use code from [this repository](https://github.com/lmullen/mullenMisc/blob/master/R/jitter-latlong.R) to jitter the latitude and longitude, so that multiple points per city can be seen.  The map is interactive: select data points by year to view the geographic trends in job listings!

```{r plot stack overflow data dump jobs on map, message=FALSE,fig.align='center',fig.height=8,fig.width=10,warning=FALSE,echo=FALSE}
# Prepare for plotting
stackjobs.df <- split(stackjobs, stackjobs$year)
l <- leaflet() %>% addTiles()

colors<-c("pink","red","orange","lightgreen","green","lightblue","blue","purple")

names(stackjobs.df) %>%
  purrr::walk( function(df) {
    l <<- l %>%
      addAwesomeMarkers(data=stackjobs.df[[df]],
                          lng=~lon, lat=~lat,
                          icon = awesomeIcons(icon = '',iconColor = 'black',
                                              library = 'ion',markerColor = colors[as.numeric(df)-2009]),
                          #label=~as.character(mag),
                          #popup=~as.character(mag),
                          group = df#,
                          #clusterOptions = markerClusterOptions(removeOutsideVisibleBounds = F),
                          #labelOptions = labelOptions(noHide = F,
                          #                             direction = 'auto'))
      )
  })

# Plot jobs!
l %>% setView(lng = -100.11,lat=40, zoom = 4) %>%
  addLayersControl(
    overlayGroups = names(stackjobs.df),
    options = layersControlOptions(collapsed = FALSE)
  ) %>% 
  addLegend("bottomleft",title="Year",colors=colors,labels=2010:2017)
```

Here is a plot (yes, I figured out how to use `ggplot`...) that shows trends in number of jobs posted by US region over the past several years.
```{r jobs by region and time, echo=FALSE, warning=FALSE,message=FALSE,fig.align='center',fig.height=8,fig.width=10}
state.data=data.frame(StateCode=state.abb,Division=state.division) #%>% 
  #inner_join(data.frame(Division=levels(state.division),colors=rainbow(9)))

division_trend<-stackjobs %>% 
  inner_join(state.data) %>% 
  group_by(year,Division) %>% 
  count()# %>% 
  #inner_join(state.data)
  
p = ggplot(division_trend,aes(x=year,y=n,group=Division,colour=Division))
p + geom_line()
```

## 3. Skills that Employers are Looking For
Here is a word cloud of technical skill tags from the Stack Overflow job listings (data dump, not manual scraping):
```{r stack overflow wordcloud, echo=FALSE,cache=TRUE,eval=TRUE, fig.align='center', fig.height=4.5,fig.width=4.5}
#d = data.frame(word = names(table(unlist(about_job$tags))), freq=as.numeric(table(unlist(about_job$tags))))

unique_jobs<-stackjobs %>% 
   filter(!duplicated(JobId))
tags<-(unlist(str_split(unique_jobs$Tags," "))) %>% str_replace_all("-"," ")

d = data.frame(word = names(table(tags)), freq = as.numeric(table(tags)))

set.seed(2)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

## More EDA:
```{r eda plots, echo=FALSE,eval=TRUE, fig.align='center', fig.height=8,fig.width=10}
exp<-strsplit(about_job$experience,", ")
kable(t(as.matrix(sort(table(unlist(exp)),decreasing=TRUE))),caption="Levels of Experience Requested")

time_trend<-stackjobs %>% 
  group_by(month,year) %>% 
  count() %>% 
  arrange(year,month) 

plot(1:nrow(time_trend),time_trend$n,type='l',xaxt='n',xlab=" ",ylab="Number of Jobs Per Month")
axis(1,at=1:nrow(time_trend),labels=paste0(time_trend$month,"-",time_trend$year),las=2,cex.axis=.6)

fewest_posts = time_trend%>% 
  group_by(year) %>% 
  slice(which.min(n))

kable(fewest_posts,caption="Months with the fewest job postings by year")
```


## Things to do next: 
- *start writing* - write up introduction/methods used
    + Introduction: Increased demand for data scientists, hot new field, Stack Overflow's popularity
    + Methods
    + Data: Web scraping, data obtained directly from SO
    + Results
    + Discussion/Limitations
- scrape data from skills section:
    + education (Masters vs. PhD) and preferred departments of degrees
    + years of experience
    + unique skill requests
    + more programming skills
- **LOOK AT GLASS DOOR API?** - I'm having trouble figuring out how to use this... I can only get aggregate level data, not specific data on individual job postings

### Analysis Plan:
- Look at trends in programming over time (while jobs only last about 4 weeks on Stack Overflow, I now have access to 8 years of postings...)
    + consider limitation: hiring season/scraping bias
    + undersampling/length bias - jobs in higher demand
- PCA clustering - skills by sector
- Are the public companies more likely to post more jobs? Private companies? Bigger companies?
- Compare job postings from two different job boards

### Limitations:
- Reproducibility issue: data are unique to when they are scraped, and I'm obtaining data from a data dump
    + list when data were pulled in report, save csv
    + one option: scrape data every so often, saving csv files, take union of rows to maximize number of postings
- Maybe not generalizable: potential bias in just looking at Stack Overflow, could attract job postings from certain types of industries
