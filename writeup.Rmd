---
title: "Trends in Data Science Job Postings on Stack Overflow"
author: "Benjamin Ackerman"
date: "October 9, 2017"
mainfont: Times New Roman
output: 
  pdf_document:
    latex_engine: xelatex
fontsize: 12pt
geometry: margin=1in
---

```{r setup, echo=FALSE, warning=FALSE,message=FALSE,results='hide'}
packages = c("devtools","qdapRegex","knitr","dplyr","kableExtra","ggmap","leaflet","stringr","tidyr","tm","SnowballC","wordcloud","RColorBrewer","rebus","maps","grid","cowplot","RgoogleMaps","ggplot2")
lapply(packages,library,character.only=TRUE)
```

```{r test setup, include=FALSE}
opts_chunk$set(dev = 'pdf')
```

```{r read and clean stack overflow data dump, eval=FALSE, echo=FALSE}
### This chunk contains the code used to clean and extract data by reading in the original RDS file from David Robinson.  To save time, this code doesn't run, and the cleaned data are already saved in the _____

# Read in original data
newdat<-readRDS('../data_scientist.rds')

# Split up the date into year, month, day
newdat[,c("year","month","day")]<-str_split(newdat$DatePosted,"-",simplify=TRUE)

# Clean Jobs that have multiple locataions 
repeats<-unique(newdat$JobId[which(duplicated(newdat$JobId))])
repeat_locations<-sapply(1:length(repeats),function(x){
   str_split(newdat$LocationString[which(newdat$JobId == repeats[x])],"; ",simplify=TRUE)[1,]
})
newdat$LocationString[which(newdat$JobId %in% repeats)] = unlist(repeat_locations)

# Clean locations a little more
newdat$LocationString = str_replace(newdat$LocationString,"London, England","London, UK")

# Geocode locations to plot them
latlon<-geocode(newdat$LocationString,output='latlon')
newdat = cbind(newdat,latlon)

#newdat$lat = jitter_latlong(latlon$lat,type='lat')
#newdat$lon = jitter_latlong(latlon$lon,type='long',newdat$lat)

### Regular expressions to find strong/ideal requirements, and lines in requirement decriptions to delete
strong=or("[Ii]"%R%"deal"%R%optional("ly"),
   "[Pp]"%R%"refer"%R%optional(or("s","red")))

delete = or(exactly("\\"%R%"r"%R%"\\"%R%"n"),
            exactly(optional("/")%R%or("p","li","ul","em","br","strong","span","sup","rd","blockquote")),
            exactly(or("",":",": ")))

# Functions to clean requirements data
get_requirements = function(x){
  reqs = unlist(str_split(newdat$Requirements[x],or("<",">"))) %>% 
  str_replace_all("&nbsp;","") %>%
  str_replace_all("&ldquo;","'") %>% 
  str_replace_all(or("&rdquo;","&rsquo;"),"'") %>% 
  str_replace_all("&nbsp;","") %>% 
  str_replace_all("&middot;","") %>% 
  str_replace_all("&amp;","&") %>% 
  str_replace_all("&bull; ","") %>%
  str_replace_all("\n ","") %>%
  str_replace_all("&ndash;","-") %>%
  str_replace_all("\\"%R%"r"%R%"\\"%R%"n","") %>% 
  str_replace(START%R%or("- ","-"),"") %>% 
  str_replace(START%R%optional("o")%R%" ","")
  
  reqs[!str_detect(reqs,delete)]
  #reqs[-str_detect(reqs,delete)]
  #reqs[str_detect(reqs,START%R%char_class("A-Z"))]
  #reqs[str_detect(reqs,strong)]
  #reqs[str_detect(reqs,or(":",": ") %R% END)]
}

get_description = function(x){
  description = unlist(str_split(newdat$Description[x],or("<",">"))) %>% 
    str_replace_all("&nbsp;","") %>%
    str_replace_all("&ldquo;","'") %>% 
    str_replace_all(or("&rdquo;","&rsquo;"),"'") %>% 
    str_replace_all("&nbsp;","") %>% 
    str_replace_all("&middot;","") %>% 
    str_replace_all("&amp;","&") %>% 
    str_replace_all("&bull; ","") %>%
    str_replace_all("\n ","") %>%
    str_replace_all("&ndash;","-") %>%
    str_replace_all("\\"%R%"r"%R%"\\"%R%"n","") %>% 
    str_replace(START%R%or("- ","-"),"") %>% 
    str_replace(START%R%optional("o")%R%" ","")
  
  description[!str_detect(description,delete)]
}

#Row numbers of jobs with requirements and description sections:
req_nums = which(!is.na(newdat$Requirements))
desc_nums = which(!is.na(newdat$Description))
desc_nums = desc_nums[-req_nums]

# Get Requirements Sections cleaned:
reqs<-lapply(req_nums,get_requirements)
# Get Descriptions Section cleaned, and *only* look at descriptions where requirements are missing:
desc<-lapply(desc_nums,get_description)

# Regular expression to find Bachelors degrees:
bachelors_degree <- or("[Bb]achelor","BA" %R% optional("/")%R% "BS", "B" %R% optional('.') %R% "S","B" %R% optional('.') %R% "A")
# Do not include these though!
no_bachelor<-or("VBA","RDMBS","Hive-BA","BAU","BART")

# Extract lines from the requirements and descriptions that mention Bachelors degrees:
bachelors_req<-lapply(reqs, function(x){x[str_detect(x,bachelors_degree) & !str_detect(x,no_bachelor)]})
bachelors_desc<-lapply(desc, function(x){x[str_detect(x,bachelors_degree) & !str_detect(x,no_bachelor)]})

newdat$bachelors = 0
newdat$bachelors[c(req_nums[lapply(bachelors_req,length)>0],desc_nums[lapply(bachelors_desc,length)>0])
] = 1

# Regular expression to find Masters degrees:
masters_degree <- or("[Mm]" %R% "aster" %R% optional("'") %R% "s","MSc","M"%R%optional(DOT)%R%optional(SPC)%R%"S"%R%optional(DOT))
# Do not include these though!
no_masters = or("RDBMS",case_insensitive("ms")%R%optional(SPC)%R%or(case_insensitive("sql"),case_insensitive("excel"),case_insensitive("word"),case_insensitive("power"),case_insensitive("share"),case_insensitive("project"),case_insensitive("SSIS"),case_insensitive("office"),case_insensitive("access"),case_insensitive("report")),"MSAs","MSMQ","MSKCC","RDMS")

# Extract lines from the requirements and descriptions that mention Bachelors degrees:
masters_req<-lapply(reqs, function(x){x[str_detect(x,masters_degree) & !str_detect(x,no_masters)]})
masters_desc<-lapply(desc, function(x){x[str_detect(x,masters_degree) & !str_detect(x,no_masters)]})

newdat$masters = 0
newdat$masters[c(req_nums[lapply(masters_req,length)>0],desc_nums[lapply(masters_desc,length)>0])
] = 1

# Regular expression to find PhD degrees:
phd_degree <- or("P" %R% optional(DOT) %R% "h" %R% optional(DOT) %R%optional(SPC)%R% or("D","d"),"[Dd]octorate")

# Extract lines from the requirements and descriptions that mention Bachelors degrees:
phds_req<-lapply(reqs, function(x){x[str_detect(x,phd_degree)]})
phds_desc<-lapply(desc, function(x){x[str_detect(x,phd_degree)]})

newdat$phd = 0
newdat$phd[c(req_nums[lapply(phds_req,length)>0],desc_nums[lapply(phds_desc,length)>0])
] = 1

# Create a variable that indicates the highest degree listed in the job listing:
newdat$highest_degree = ifelse(newdat$phd == 1, "phd",ifelse(newdat$masters==1, "masters",ifelse(newdat$bachelors==1, "bachelors",NA)))

## Find STEM majors:
stem_degrees<-readLines("http://stemdegreelist.com/stem-degree-list/")
stem<-as.character(na.omit(unlist(rm_between(stem_degrees,"<li>","</li>",extract=TRUE))))
stem = c(stem,"applied math","CS","operation research","Computational Physics","Biostatistics")

# Extract all of the requested majors based on the STEM
majors_wanted_req = majors_wanted_desc = list()
for(i in 1:length(reqs)){
  majors_wanted_req[[i]]=unique(names(which(sapply(stem, function(x){
    any(str_detect(reqs[[i]],"\\b"%R%case_insensitive(x)%R%"\\b"))})==TRUE)))
}
for(i in 1:length(desc)){
  majors_wanted_desc[[i]]=unique(names(which(sapply(stem, function(x){
    any(str_detect(desc[[i]],"\\b"%R%case_insensitive(x)%R%"\\b"))})==TRUE)))
}

# If "Applied Mathematics" is listed, get rid of "Mathematics" from the list
for(i in which(lapply(majors_wanted_req,length)>0)){
    if(sum(sapply(mathstrings,function(x){str_detect(majors_wanted_req[[i]],case_insensitive(exactly(x)))}))==2){
majors_wanted_req[[i]]=majors_wanted_req[[i]][-which(str_detect(majors_wanted_req[[i]],case_insensitive(exactly("Mathematics"))))]}}

for(i in which(lapply(majors_wanted_desc,length)>0)){
    if(sum(sapply(mathstrings,function(x){str_detect(majors_wanted_desc[[i]],case_insensitive(exactly(x)))}))==2){
majors_wanted_desc[[i]]=majors_wanted_desc[[i]][-which(str_detect(majors_wanted_desc[[i]],case_insensitive(exactly("Mathematics"))))]}}

# Clean up "CS","applied math" and "operation research"
majors_wanted_req = lapply(majors_wanted_req,function(x){str_replace_all(x,"CS","Computer Science")})
majors_wanted_req = lapply(majors_wanted_req,function(x){str_replace_all(x,"applied math","Applied Mathematics")})
majors_wanted_req = lapply(majors_wanted_req,function(x){str_replace_all(x,"operation research","Operations Research")})

majors_wanted_desc = lapply(majors_wanted_desc,function(x){str_replace_all(x,"CS","Computer Science")})
majors_wanted_desc = lapply(majors_wanted_desc,function(x){str_replace_all(x,"applied math","Applied Mathematics")})
majors_wanted_desc = lapply(majors_wanted_desc,function(x){str_replace_all(x,"operation research","Operations Research")})

newdat$majors = NA
newdat$majors[req_nums] = majors_wanted_req
newdat$majors[desc_nums] = majors_wanted_desc
newdat$majors[which(lapply(newdat$majors,length)==0)]=NA

# Save updated dataset as "stackjobs.rds"
saveRDS(newdat,"../stackjobs.rds")
```

## Introduction
- Increased demand for data scientists, hot new field, Stack Overflow's popularity

## Research Aim
The purpose of this paper is to examine trends in job postings for "data scientists" on the Stack Overflow job board.  This involves determining the most common computing skills that employers look for, along with their preferences of degree types and areas of study.  This paper will also locate geographic regions where data science jobs are in highest demand, and if there are substantial differences in job characteristics by location.  Finally, trends of these characteristics of job listings will be explored over time.

## Methods
### Data Collection
Data were made privately available upon request from David Robinson, a Data Scientist at Stack Overflow.  The provided data consist of information from jobs posted on the Stack Overflow job board that either have "data scientist" or "data analyst" in their title between August 25, 2010 and September 25, 2017.  While company names were censored from the data, the following attributes of each posting were provided in a data frame: job title, original posting date (YEAR-MM-DD), associated tags indicating relevant skills, job location (City, State, Country), salary (when included), and the full text of job descriptions and requirements.

While a fair amount of variables were already provided in a dataframe, additional information was extracted from the data and cleaned.  The `geocode` function in the `ggmap` package was used to gather latitude and longitude coordinates for each job location. Preferences of academic backgrounds were extracted from the job requirements section.  This included any mentions of type of degree (Bachelors, Masters, PhD) along with mentions of favorable majors and departments.  To detect relevant majors, a dictionary was compiled using a comprehensive list of STEM fields provided by [**Stemdegreelist.com**](http://stemdegreelist.com/stem-degree-list/).  Additionally, for jobs that mentioned multiple degrees (i.e, "Bachelor's degree required, Master's degree preferred"), the "highest degree preferred" for a job listing was determined.  For listings that did not provide job requirement sections, the job descriptions section was used to check for these attributes.

### Exploratory Data Analysis
Exploratory data analysis was conducted to summarize the most commonly listed attributes in the job postings.  Skill tags, areas of study, and job locations were tabulated across all postings and ranked to determine the most common skills sought by employers, and where the most employment opportunities were geographically located.  Hex maps were generated to view the distribution of the number of jobs posted by geographic location.  Number of job postings were also tabulated by month and year to determine if there were any changes in frequency of postings over time.

### Statistical Analysis
In order to assess any differences in jobs by location, proportions of jobs that offer visa sponsorship, allow remote work, and assist with relocation were compared between jobs listed in the US and Europe, the two geographic regions with the highest numbers of job listings, using two-sample t-tests.  The distributions of highest degree preferred were compared across regions with a Pearson's Chi-squared test.

## Results
```{r load data,echo=FALSE}
stackjobs = readRDS("../stackjobs.rds")
```

```{r jitter latitude and longitude functions,echo=FALSE}
length_of_degree <- function(degree, type = c("lat", "long")) {
  type <- match.arg(type)
  length_at_equator <- 110.5742727 # in kilometers
  if (type == "long") {
    cos(degree * (2 * pi) / 360) * length_at_equator
  } else if (type == "lat") {
    length_at_equator
  }
}
jitter_latlong <- function(coord, type = c("lat", "long"), latitude, km = 1) {
  type = match.arg(type)
  if(missing(latitude) & type == "lat") latitude <- coord
  km_per_degree <- length_of_degree(latitude, type = type)
  degree_per_km <- 1 / km_per_degree
  coord + (runif(1, min = -1, max = 1) * degree_per_km * km)
}
jitter_latlong <- Vectorize(jitter_latlong,
                            vectorize.args = c("coord", "latitude"))
```

```{r popular majors, echo=FALSE, results='tex',eval=FALSE}
pop_majors = data.frame(sort(table(unlist(stackjobs$majors)),decreasing=TRUE))
names(pop_majors) = c("Major","n")
kable(pop_majors[1:10,],caption="10 most common majors listed in job postings")
```

### Most Popular Attributes of Job Listings from 2010-2017:

```{r top tags areas of study and cities, echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.height=6,fig.width=8}
toptags = stackjobs %>% 
  ungroup() %>% 
  .$Tags %>% 
  str_split(" ") %>% 
  unlist() %>% 
  str_replace_all("-"," ") %>% 
  data.frame(tags=.,stringsAsFactors=FALSE) %>% 
  group_by(tags) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  filter(!is.na(tags)) %>% 
  filter(!tags %in% c("statistics","bigdata","javascript","data science"))

tags = ggplot(toptags[1:10,], aes(reorder(tags,n),n)) + geom_col() + coord_flip() + labs(y="Count",x="")+theme(axis.title.x=element_text(size=11),
               axis.title.y=element_text(size=11),
               axis.text.x = element_text(size=10),
               axis.text.y = element_text(size=10))

topmajors = stackjobs %>% 
  .$majors %>% 
  unlist() %>% 
  data.frame(majors=.,stringsAsFactors=FALSE) %>% 
  group_by(majors) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  filter(!is.na(majors))

majors = ggplot(topmajors[1:10,], aes(reorder(majors,n),n)) + geom_col() + coord_flip() + labs(y="Count",x="")+theme(axis.title.x=element_text(size=11),
               axis.title.y=element_text(size=11),
               axis.text.x = element_text(size=10),
               axis.text.y = element_text(size=10))

#cowplot::plot_grid(tags,majors, labels = c("Top 10 Tags in Job Listings","Top 10 Areas of Study in Job Listings"), align = "v",scale = 0.9,hjust=-.25)

toplocations = stackjobs %>% 
  .$LocationString %>% 
  unlist() %>% 
  data.frame(locations=.,stringsAsFactors=FALSE) %>% 
  group_by(locations) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  filter(!is.na(locations))

locations = ggplot(toplocations[1:10,], aes(reorder(locations,n),n)) + geom_col() + coord_flip() + labs(y="Count",x="")+theme(axis.title.x=element_text(size=11),
               axis.title.y=element_text(size=11),
               axis.text.x = element_text(size=10),
               axis.text.y = element_text(size=10))

top_row = plot_grid(tags,majors, labels = c("Top 10 Tags in Job Listings","Top 10 Areas of Study in Job Listings",""),scale = 0.9,hjust=-.15,label_size=12)
bottom_row = plot_grid(locations,labels=c("Top 10 Cities with the Most Job Listings"),scale=.9,hjust=-.35,label_size=12)
plot_grid(top_row,bottom_row,ncol=1)
```

```{r plot US and EU maps,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.height=4,fig.width=10}
mapdat_us = stackjobs %>% 
  group_by(LocationString) %>% 
  count() %>% 
  inner_join(stackjobs,by="LocationString") %>% 
  filter(CountryCode == "US") %>% 
  mutate(region = "USA")

us=ggplot()+borders(database="state")+geom_hex(data=mapdat_us, mapping=aes(x=lon,y=lat), bins=15,alpha=.8)+scale_fill_gradient(low = "yellow", high = "red")+theme(
              legend.position="none",
              axis.title.x = element_blank(),
               axis.line = element_blank(),
               axis.text.x=element_blank(),
               axis.ticks.x=element_blank(),
               axis.title.y = element_blank(),
               axis.text.y=element_blank(),
               axis.ticks.y=element_blank())

mapdat_eu = stackjobs %>% 
  group_by(LocationString) %>% 
  count() %>% 
  inner_join(stackjobs,by="LocationString") %>%
  filter(CountryCode %in% c("AT","BE","BG","HR","CY","CZ","DK","EE","FI","FR","DE","GR","HU","IE","IT","LV","LT","LU","MT","NL","PL","PT","RO","SK","SI","ES","SE","GB")) %>% 
  mutate(region = "EU")

europe=ggplot()+borders(database="world",xlim=c(-10,25),ylim=c(40,70))+geom_hex(data=mapdat_eu, mapping=aes(x=lon,y=lat), bins=15,alpha=.8)+scale_fill_gradient("# of Jobs",low = "yellow", high = "red")+theme(
               axis.title.x = element_blank(),
               axis.line = element_blank(),
               axis.text.x=element_blank(),
               axis.ticks.x=element_blank(),
               axis.title.y = element_blank(),
               axis.text.y=element_blank(),
               axis.ticks.y=element_blank(),
               legend.title=element_text(size=10),
               legend.text=element_text(size=10))

plot_grid(us,europe, labels = c(paste0("USA (n=",nrow(mapdat_us),")"), paste0("Europe (n=",nrow(mapdat_eu),")")), align = "v")
```


```{r model differences between US and EU jobs, echo=FALSE,results='tex'}
# Group data by Region: US or Europe
diff_analysis = stackjobs %>% 
  filter(CountryCode %in% c("US","AT","BE","BG","HR","CY","CZ","DK","EE","FI","FR","DE","GR","HU","IE","IT","LV","LT","LU","MT","NL","PL","PT","RO","SK","SI","ES","SE","GB")) %>% 
  mutate(region = ifelse(CountryCode == "US","USA","EU"))

# Tabulate differences for Visa Sponsorship, Remote Work, and Offering Relocation
diffs=as.matrix(diff_analysis %>% 
  group_by(region) %>% 
  summarise(visa = sum(OffersVisaSponsorship),
            remote = sum(AllowsRemote),
            relocation = sum(OffersRelocation),
            n=n()) %>% 
  select(-region))

job_diffs = matrix(paste0(diffs[,-4]," (", round(diffs/diffs[,4]*100,1)[,-4],"%)"),ncol=2,byrow=TRUE)[,c(2,1)]

# Tabulate differences in highest degree required
tab<-with(diff_analysis,table(region,highest_degree))
degree_diffs = matrix(paste0(t(tab)," (", round(t(prop.table(tab,1))*100,1),"%)")
,ncol=2)[,c(2,1)]

# Find how many per region are missing highest degree information
missing = as.numeric(with(diff_analysis,table(region,is.na(highest_degree)))[,1])[c(2,1)]

# Test if there is a difference between the proportions for US and Europe, and report p-value
p=rep(NA,3)
for(i in 1:3){
  p[i] = prop.test(diffs[,i],diffs[,4])$p.value
}

p = c(p,chisq.test(tab)$p.value) %>% 
  format(scientific = TRUE,digits = 3) %>% 
  c(rep("",3))

# Put the table together
diff_tab = rbind(job_diffs,c("",""),degree_diffs)
diff_tab=cbind(diff_tab,p)
colnames(diff_tab) = c("USA","Europe","P-value")
#colnames(diff_tab) = c(paste0("USA (n= ",diffs[2,4],")"),paste0("Europe (n= ",diffs[1,4],")"),"P-value")
row.names(diff_tab) = c("Visa Sponsorship","Allows Remote Work", "Offers Relocation","Highest Degree Preferred:[note]", "Bachelors","Masters","PhD")

# Print the table
kable(diff_tab, format = "latex", booktabs = T,caption="Differences in Job Listings in the US vs. Europe") %>%
kable_styling(latex_options=c("striped","hold_position")) %>% add_footnote(paste0("Due to missingness, percents are calculated from totals of ",missing[1]," for USA and ",missing[2]," for Europe."), notation = "number")

# model = glm(region == "USA" ~ OffersVisaSponsorship + AllowsRemote +  highest_degree,family='binomial',data=diff_analysis)
```

### Changes in Attributes of Job Listings over the Last 5 Years (2013-2017):

```{r trends over time,echo=FALSE,results='tex'}
tag_yr <- function(yr){
  toptags = stackjobs %>% 
    filter(year == yr) %>% 
    ungroup() %>% 
    .$Tags %>% 
    str_split(" ") %>% 
    unlist() %>% 
    str_replace_all("-"," ") %>% 
    data.frame(tags=.,stringsAsFactors=FALSE) %>% 
    group_by(tags) %>% 
    count() %>% 
    arrange(desc(n)) %>% 
    filter(!is.na(tags)) %>% 
    filter(!tags %in% c("statistics","bigdata","javascript","data science")) %>% 
    head(n=10) %>% 
    .$tags
}

tag_trend = sapply(2013:2017,tag_yr)
colnames(tag_trend) = 2013:2017
row.names(tag_trend) = 1:10

kable(tag_trend,format = "latex", booktabs = T,caption="Top 10 Tags in Job Listings by Year")%>%kable_styling(latex_options=c("striped","hold_position"))

major_yr <- function(yr){
  stackjobs %>% 
    filter(year == yr) %>% 
    .$majors %>% 
    unlist() %>% 
    data.frame(majors=.,stringsAsFactors=FALSE) %>% 
    group_by(majors) %>% 
    count() %>% 
    arrange(desc(n)) %>% 
    filter(!is.na(majors)) %>% 
    head(n=10) %>% 
    .$majors
}

major_trend = sapply(2013:2017,major_yr)
colnames(major_trend) = 2013:2017
row.names(major_trend) = 1:10

#kable(major_trend,format = "latex", booktabs = T,caption="Top 10 Areas of Study in Job Listings by Year")%>%
#kable_styling(latex_options="hold_position")

city_yr <- function(yr){
  cities =stackjobs %>%
    filter(year == yr) %>% 
    .$LocationString %>% 
    unlist() %>% 
    data.frame(locations=.,stringsAsFactors=FALSE) %>% 
    group_by(locations) %>% 
    count() %>% 
    arrange(desc(n)) %>% 
    filter(!is.na(locations)) %>% 
    head(n=10) %>% 
    .$locations %>% 
    str_split(", ",simplify=TRUE)
  cities[,1]
}

city_trend = sapply(2013:2017,city_yr)
colnames(city_trend) = 2013:2017
row.names(city_trend) = 1:10

kable(city_trend,format = "latex", booktabs = T,caption="Top 10 Cities with the Most Job Listings by Year")%>%kable_styling(latex_options=c("striped","hold_position"))

```

\clearpage

## Discussion/Limitations

Discussion:

- More quantitative fields dominate top areas of study requested
- While Python has consistently been most tagged skill, R and machine learning have become increasingly more important to data science jobs in the last three years (as they move up the rankings)
- Most US jobs are in big cities like New York, San Francisco, Boston, and Chicago.  While most jobs overall are located in the US, in the last two years, European cities have been becoming increasingly more popular.
- There are significant differences between US and European jobs: US allows more remore work, and more jobs for people with Bachelors degrees, while European jobs sponsor more visas and offer more compensation for relocation.

Limitations:

- results do not generalize to all data science job boards - Stack Overflow could attract jobs from particular industries, could be biased in this way
- skills are detected through tags and not through what is mentioned in the text

## Conclusion